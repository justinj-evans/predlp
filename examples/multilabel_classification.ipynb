{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraining label counts in IMBD News Groups using LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/justinj-evans/predlp/blob/master/examples/multilabel_classification_imdb.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example demonstrates how to impose a label constraint using the python package *predlp*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this tutorial is from scikit-learn's multilabel classification dataset '20newsgroups'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load open-source dataset\n",
    "newsgroups = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# create dataframe\n",
    "data = pd.DataFrame({'text': newsgroups['data'], 'target': newsgroups['target']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impose class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a class imbalance?**\n",
    "Class imbalance refers to a situation in a dataset where the distribution of target classes is not uniform or is significantly skewed. Class imbalance is challenging for models to classify because it skews the underlying data distribution, which significantly impacts sampling and statistical inference. \n",
    "\n",
    "**Why impose a class imbalance?**\n",
    "Here, we're trying to generate a realistic scenario in a multilabel classification problem and determine if the package *predlp* can improve class representation by imposing label constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class imbalance\n",
    "# Count the occurrences of each target class\n",
    "class_counts = Counter(data['target'])\n",
    "\n",
    "# Find the top two classes with the largest counts and the bottom class with the least count\n",
    "top_two_classes = [item[0] for item in class_counts.most_common(2)]\n",
    "bottom_class = min(class_counts, key=class_counts.get)\n",
    "\n",
    "# Filter the dataframe to include only the selected classes\n",
    "selected_classes = top_two_classes + [bottom_class]\n",
    "data_class_imbalanced = data[data['target'].isin(selected_classes)]\n",
    "\n",
    "# Keep class names for visualizing results\n",
    "selected_class_names = [f\"{[class_idx]}-{newsgroups['target_names'][class_idx]}\" for class_idx in selected_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train/Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is stratified sampling?**\n",
    "Stratified sampling ensures that train and test splits preserve the proportion of key variables (e.g., target classes), reducing sampling variability and improving representativeness. It assumes the stratification variable is relevant, strata are well-defined and sufficiently large, and the data distribution reflects what the model will encounter during deployment.\n",
    "\n",
    "**Why use stratified sampling?**\n",
    "In an ideal modeling scenario, if your train/test split is 50:50 and you stratify by target classes, the model should predict class counts proportional to those it was trained on. For instance, if the training dataset contains 100 examples of \"dogs\" and 10 examples of \"cats,\" the model, when tested on a dataset with an identical class makeup, should ideally predict the same proportions.\n",
    "    \n",
    "However, we never build perfect models and class imbalance will favor majority classes if two classes are semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_class_imbalanced['text'], data_class_imbalanced['target'], test_size=0.5, random_state=42, stratify=data_class_imbalanced['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Counts: {10: 199, 15: 199, 19: 126}\n",
      "Test Class Counts: {10: 200, 15: 199, 19: 125}\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate that train/test datasets have the same class counts\n",
    "print(f\"Train Class Counts: {dict(OrderedDict(sorted(Counter(y_train).items())))}\")\n",
    "print(f\"Test Class Counts: {dict(OrderedDict(sorted(Counter(y_test).items())))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Model: Logistic Regression with OvR Strategy\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predictions with Probabilities\n",
    "y_pred_proba = model.predict_proba(X_test_vec)\n",
    "y_pred = model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that class [19]-talk.religion.misc goes from an expected class count of 125 (23% of records) in the test dataset to a predicted class count of 67 (12% of records)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Class Counts: {10: 200, 15: 199, 19: 125}\n",
      "Predicted Test Class Counts: {10: 223, 15: 234, 19: 67}\n"
     ]
    }
   ],
   "source": [
    "test_label_counts = dict(Counter(y_test))\n",
    "pred_label_counts = dict(OrderedDict(sorted(Counter(y_pred).items())))\n",
    "\n",
    "print(f\"Test Class Counts: {test_label_counts}\")\n",
    "print(f\"Predicted Test Class Counts: {pred_label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "      [10]-rec.sport.hockey       0.88      0.98      0.93       200\n",
      "[15]-soc.religion.christian       0.82      0.96      0.89       199\n",
      "    [19]-talk.religion.misc       0.96      0.51      0.67       125\n",
      "\n",
      "                   accuracy                           0.86       524\n",
      "                  macro avg       0.89      0.82      0.83       524\n",
      "               weighted avg       0.88      0.86      0.85       524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(list(y_test), list(y_pred), target_names=selected_class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, while our classifier achieves high precision for class [19]-talk.religion.misc, it misses a significant portion of the expected data (recall at 0.51)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label constraints using the package *predlp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using *predlp* to enforce predefined label constraints in our model's predictions.  We feed in the model's predicted probabilities for each label and example (pred_probs) along with our label constraint. The package uses these linear programming constraints to maximizes the model's cumulative confidence score.\n",
    "\n",
    "**Assumptions**  \n",
    "Is it likely to know the exact distribution of new data (e.g., the test dataset) in advance? No. However, practitioners may estimate a plausible range of distributions based on prior experience or through statistical inference. These ranges can serve as constraints to guide model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Score: 318.29752711365853\n"
     ]
    }
   ],
   "source": [
    "from predlp.solver import pred_prob_lp\n",
    "pred_after_lp = pred_prob_lp(class_names=selected_classes, label_counts= test_label_counts, pred_probs=y_pred_proba)\n",
    "pred_after_lp_label_counts = Counter(pred_after_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Class Counts: {10: 200, 15: 199, 19: 125}\n",
      "Predicted Test Class Counts: {10: 200, 15: 199, 19: 125}\n"
     ]
    }
   ],
   "source": [
    "# demonstrate constraint successful \n",
    "print(f\"Test Class Counts: {dict(test_label_counts)}\")\n",
    "print(f\"Predicted Test Class Counts: {dict(pred_after_lp_label_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "      [10]-rec.sport.hockey       0.98      0.98      0.98       200\n",
      "[15]-soc.religion.christian       0.91      0.91      0.91       199\n",
      "    [19]-talk.religion.misc       0.85      0.85      0.85       125\n",
      "\n",
      "                   accuracy                           0.92       524\n",
      "                  macro avg       0.91      0.91      0.91       524\n",
      "               weighted avg       0.92      0.92      0.92       524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(list(y_test), list(pred_after_lp), target_names=selected_class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see *predlp* improved recall for our minitory class and resulted in a higher f1-score for all classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
